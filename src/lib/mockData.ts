import { DAG, DAGRun, TaskInstance } from "@/types/airflow";

export const mockDAGs: DAG[] = [
  {
    dag_id: "example_bash_operator",
    description: "Example DAG that runs bash commands",
    file_token: "abc123",
    fileloc: "/opt/airflow/dags/example_bash_operator.py",
    is_active: true,
    is_paused: false,
    is_subdag: false,
    last_expired: null,
    last_parsed_time: new Date().toISOString(),
    last_pickled: null,
    max_active_runs: 16,
    max_active_tasks: 16,
    next_dagrun: new Date(Date.now() + 3600000).toISOString(),
    next_dagrun_create_after: new Date(Date.now() + 3600000).toISOString(),
    next_dagrun_data_interval_end: null,
    next_dagrun_data_interval_start: null,
    owners: ["airflow"],
    pickle_id: null,
    root_dag_id: null,
    schedule_interval: { __type: "CronExpression", value: "0 * * * *" },
    scheduler_lock: null,
    tags: [{ name: "example" }, { name: "bash" }],
    timetable_description: "At minute 0",
  },
  {
    dag_id: "example_python_operator",
    description: "Example DAG that runs Python functions",
    file_token: "def456",
    fileloc: "/opt/airflow/dags/example_python_operator.py",
    is_active: true,
    is_paused: true,
    is_subdag: false,
    last_expired: null,
    last_parsed_time: new Date().toISOString(),
    last_pickled: null,
    max_active_runs: 16,
    max_active_tasks: 16,
    next_dagrun: null,
    next_dagrun_create_after: null,
    next_dagrun_data_interval_end: null,
    next_dagrun_data_interval_start: null,
    owners: ["airflow"],
    pickle_id: null,
    root_dag_id: null,
    schedule_interval: { __type: "CronExpression", value: "0 0 * * *" },
    scheduler_lock: null,
    tags: [{ name: "example" }, { name: "python" }],
    timetable_description: "At 00:00",
  },
  {
    dag_id: "etl_data_pipeline",
    description: "ETL pipeline for data processing",
    file_token: "ghi789",
    fileloc: "/opt/airflow/dags/etl_data_pipeline.py",
    is_active: true,
    is_paused: false,
    is_subdag: false,
    last_expired: null,
    last_parsed_time: new Date().toISOString(),
    last_pickled: null,
    max_active_runs: 4,
    max_active_tasks: 8,
    next_dagrun: new Date(Date.now() + 7200000).toISOString(),
    next_dagrun_create_after: new Date(Date.now() + 7200000).toISOString(),
    next_dagrun_data_interval_end: null,
    next_dagrun_data_interval_start: null,
    owners: ["data-team"],
    pickle_id: null,
    root_dag_id: null,
    schedule_interval: { __type: "CronExpression", value: "0 */6 * * *" },
    scheduler_lock: null,
    tags: [{ name: "etl" }, { name: "data" }, { name: "production" }],
    timetable_description: "Every 6 hours",
  },
  {
    dag_id: "ml_training_pipeline",
    description: "Machine learning model training pipeline",
    file_token: "jkl012",
    fileloc: "/opt/airflow/dags/ml_training_pipeline.py",
    is_active: true,
    is_paused: false,
    is_subdag: false,
    last_expired: null,
    last_parsed_time: new Date().toISOString(),
    last_pickled: null,
    max_active_runs: 1,
    max_active_tasks: 4,
    next_dagrun: new Date(Date.now() + 86400000).toISOString(),
    next_dagrun_create_after: new Date(Date.now() + 86400000).toISOString(),
    next_dagrun_data_interval_end: null,
    next_dagrun_data_interval_start: null,
    owners: ["ml-team"],
    pickle_id: null,
    root_dag_id: null,
    schedule_interval: { __type: "CronExpression", value: "0 2 * * 1" },
    scheduler_lock: null,
    tags: [{ name: "ml" }, { name: "training" }],
    timetable_description: "At 02:00 on Monday",
  },
  {
    dag_id: "data_quality_check",
    description: "Data quality validation and monitoring",
    file_token: "mno345",
    fileloc: "/opt/airflow/dags/data_quality_check.py",
    is_active: true,
    is_paused: true,
    is_subdag: false,
    last_expired: null,
    last_parsed_time: new Date().toISOString(),
    last_pickled: null,
    max_active_runs: 8,
    max_active_tasks: 16,
    next_dagrun: null,
    next_dagrun_create_after: null,
    next_dagrun_data_interval_end: null,
    next_dagrun_data_interval_start: null,
    owners: ["data-team"],
    pickle_id: null,
    root_dag_id: null,
    schedule_interval: { __type: "CronExpression", value: "*/30 * * * *" },
    scheduler_lock: null,
    tags: [{ name: "quality" }, { name: "monitoring" }],
    timetable_description: "Every 30 minutes",
  },
];

export const mockDAGRuns: Record<string, DAGRun[]> = {
  example_bash_operator: [
    {
      conf: {},
      dag_id: "example_bash_operator",
      dag_run_id: "scheduled__2024-01-15T10:00:00+00:00",
      data_interval_end: null,
      data_interval_start: null,
      end_date: new Date(Date.now() - 1800000).toISOString(),
      execution_date: new Date(Date.now() - 3600000).toISOString(),
      external_trigger: false,
      last_scheduling_decision: null,
      logical_date: new Date(Date.now() - 3600000).toISOString(),
      note: null,
      run_type: "scheduled",
      start_date: new Date(Date.now() - 3600000).toISOString(),
      state: "success",
    },
    {
      conf: {},
      dag_id: "example_bash_operator",
      dag_run_id: "scheduled__2024-01-15T09:00:00+00:00",
      data_interval_end: null,
      data_interval_start: null,
      end_date: new Date(Date.now() - 5400000).toISOString(),
      execution_date: new Date(Date.now() - 7200000).toISOString(),
      external_trigger: false,
      last_scheduling_decision: null,
      logical_date: new Date(Date.now() - 7200000).toISOString(),
      note: null,
      run_type: "scheduled",
      start_date: new Date(Date.now() - 7200000).toISOString(),
      state: "success",
    },
    {
      conf: {},
      dag_id: "example_bash_operator",
      dag_run_id: "manual__2024-01-15T08:30:00+00:00",
      data_interval_end: null,
      data_interval_start: null,
      end_date: null,
      execution_date: new Date(Date.now() - 1800000).toISOString(),
      external_trigger: true,
      last_scheduling_decision: null,
      logical_date: new Date(Date.now() - 1800000).toISOString(),
      note: null,
      run_type: "manual",
      start_date: new Date(Date.now() - 1800000).toISOString(),
      state: "running",
    },
  ],
  etl_data_pipeline: [
    {
      conf: {},
      dag_id: "etl_data_pipeline",
      dag_run_id: "scheduled__2024-01-15T06:00:00+00:00",
      data_interval_end: null,
      data_interval_start: null,
      end_date: null,
      execution_date: new Date(Date.now() - 600000).toISOString(),
      external_trigger: false,
      last_scheduling_decision: null,
      logical_date: new Date(Date.now() - 600000).toISOString(),
      note: null,
      run_type: "scheduled",
      start_date: new Date(Date.now() - 600000).toISOString(),
      state: "running",
    },
    {
      conf: {},
      dag_id: "etl_data_pipeline",
      dag_run_id: "scheduled__2024-01-15T00:00:00+00:00",
      data_interval_end: null,
      data_interval_start: null,
      end_date: new Date(Date.now() - 18000000).toISOString(),
      execution_date: new Date(Date.now() - 21600000).toISOString(),
      external_trigger: false,
      last_scheduling_decision: null,
      logical_date: new Date(Date.now() - 21600000).toISOString(),
      note: null,
      run_type: "scheduled",
      start_date: new Date(Date.now() - 21600000).toISOString(),
      state: "failed",
    },
  ],
  ml_training_pipeline: [
    {
      conf: {},
      dag_id: "ml_training_pipeline",
      dag_run_id: "scheduled__2024-01-08T02:00:00+00:00",
      data_interval_end: null,
      data_interval_start: null,
      end_date: new Date(Date.now() - 86400000 * 7).toISOString(),
      execution_date: new Date(Date.now() - 86400000 * 7).toISOString(),
      external_trigger: false,
      last_scheduling_decision: null,
      logical_date: new Date(Date.now() - 86400000 * 7).toISOString(),
      note: null,
      run_type: "scheduled",
      start_date: new Date(Date.now() - 86400000 * 7).toISOString(),
      state: "success",
    },
  ],
};

export const mockTaskInstances: Record<string, TaskInstance[]> = {
  "example_bash_operator:scheduled__2024-01-15T10:00:00+00:00": [
    {
      dag_id: "example_bash_operator",
      dag_run_id: "scheduled__2024-01-15T10:00:00+00:00",
      duration: 5.5,
      end_date: new Date(Date.now() - 3590000).toISOString(),
      execution_date: new Date(Date.now() - 3600000).toISOString(),
      executor_config: "{}",
      hostname: "worker-1",
      map_index: -1,
      max_tries: 3,
      note: null,
      operator: "BashOperator",
      pid: 12345,
      pool: "default_pool",
      pool_slots: 1,
      priority_weight: 1,
      queue: "default",
      queued_when: new Date(Date.now() - 3600000).toISOString(),
      rendered_fields: {},
      sla_miss: null,
      start_date: new Date(Date.now() - 3595500).toISOString(),
      state: "success",
      task_id: "run_this_first",
      trigger: null,
      triggerer_job: null,
      try_number: 1,
      unixname: "airflow",
    },
    {
      dag_id: "example_bash_operator",
      dag_run_id: "scheduled__2024-01-15T10:00:00+00:00",
      duration: 10.2,
      end_date: new Date(Date.now() - 3580000).toISOString(),
      execution_date: new Date(Date.now() - 3600000).toISOString(),
      executor_config: "{}",
      hostname: "worker-1",
      map_index: -1,
      max_tries: 3,
      note: null,
      operator: "BashOperator",
      pid: 12346,
      pool: "default_pool",
      pool_slots: 1,
      priority_weight: 2,
      queue: "default",
      queued_when: new Date(Date.now() - 3590000).toISOString(),
      rendered_fields: {},
      sla_miss: null,
      start_date: new Date(Date.now() - 3590200).toISOString(),
      state: "success",
      task_id: "run_after_loop",
      trigger: null,
      triggerer_job: null,
      try_number: 1,
      unixname: "airflow",
    },
    {
      dag_id: "example_bash_operator",
      dag_run_id: "scheduled__2024-01-15T10:00:00+00:00",
      duration: 3.1,
      end_date: new Date(Date.now() - 3570000).toISOString(),
      execution_date: new Date(Date.now() - 3600000).toISOString(),
      executor_config: "{}",
      hostname: "worker-2",
      map_index: -1,
      max_tries: 3,
      note: null,
      operator: "BashOperator",
      pid: 12347,
      pool: "default_pool",
      pool_slots: 1,
      priority_weight: 3,
      queue: "default",
      queued_when: new Date(Date.now() - 3580000).toISOString(),
      rendered_fields: {},
      sla_miss: null,
      start_date: new Date(Date.now() - 3573100).toISOString(),
      state: "success",
      task_id: "this_will_skip",
      trigger: null,
      triggerer_job: null,
      try_number: 1,
      unixname: "airflow",
    },
  ],
};

export const mockTaskLogs: Record<string, string> = {
  "run_this_first": `[2024-01-15 10:00:05,123] {taskinstance.py:1234} INFO - Starting task instance
[2024-01-15 10:00:05,124] {bash.py:142} INFO - Running command: echo "run_this_first started"
[2024-01-15 10:00:05,125] {bash.py:149} INFO - Output:
[2024-01-15 10:00:05,126] {bash.py:153} INFO - run_this_first started
[2024-01-15 10:00:05,500] {bash.py:142} INFO - Running command: sleep 5
[2024-01-15 10:00:10,502] {bash.py:157} INFO - Command finished with return code 0
[2024-01-15 10:00:10,600] {taskinstance.py:1567} INFO - Marking task as SUCCESS
[2024-01-15 10:00:10,650] {local_task_job.py:156} INFO - Task finished successfully`,
  "run_after_loop": `[2024-01-15 10:00:15,200] {taskinstance.py:1234} INFO - Starting task instance
[2024-01-15 10:00:15,201] {bash.py:142} INFO - Running command: echo "run_after_loop processing"
[2024-01-15 10:00:15,205] {bash.py:149} INFO - Output:
[2024-01-15 10:00:15,206] {bash.py:153} INFO - run_after_loop processing
[2024-01-15 10:00:15,300] {bash.py:142} INFO - Running command: for i in {1..10}; do echo "Processing item $i"; sleep 1; done
[2024-01-15 10:00:16,350] {bash.py:153} INFO - Processing item 1
[2024-01-15 10:00:17,380] {bash.py:153} INFO - Processing item 2
[2024-01-15 10:00:18,410] {bash.py:153} INFO - Processing item 3
[2024-01-15 10:00:19,440] {bash.py:153} INFO - Processing item 4
[2024-01-15 10:00:20,470] {bash.py:153} INFO - Processing item 5
[2024-01-15 10:00:21,500] {bash.py:153} INFO - Processing item 6
[2024-01-15 10:00:22,530] {bash.py:153} INFO - Processing item 7
[2024-01-15 10:00:23,560] {bash.py:153} INFO - Processing item 8
[2024-01-15 10:00:24,590] {bash.py:153} INFO - Processing item 9
[2024-01-15 10:00:25,620] {bash.py:153} INFO - Processing item 10
[2024-01-15 10:00:25,700] {bash.py:157} INFO - Command finished with return code 0
[2024-01-15 10:00:25,800] {taskinstance.py:1567} INFO - Marking task as SUCCESS`,
};
